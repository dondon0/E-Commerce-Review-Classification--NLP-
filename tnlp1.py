# -*- coding: utf-8 -*-
"""TNLP1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d1Rbvr8tQbXBCAhAl4Ex8zJxFNqPcwG9

## Don Rui Tornado Rosa (Persevere 20)
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Untuk mempermudah, simpan setiap objek agar dapat digunakan untuk pemodelan maupun deployment. Gunakan library Pickle
import pickle

# %matplotlib inline
import nltk
nltk.download('stopwords')

!pip install wordcloud

!pip install sastrawi

data= pd.read_csv('reviews.csv')
data.head(5)

"""### Dari line diatas dapat dilihat cukup banyak data konten review yang null atau kosong. Menurut saya, ada beberapa langkah. Pertama menghapus data yang memiliki kolom reviewTitle dan content yang null. Tetapi, menghapus data tersebut akan mengurangi setengah dataset kita. Menurut saya lebih baik mengganti data kosong dikedua kolom tersebut menggunakan sebuah placeholder."""

data.loc[data['reviewTitle'].isnull(),'reviewTitle']= data['reviewContent']
data.loc[data['reviewContent'].isnull(),'reviewContent']= data['reviewTitle']
# data

data

data= data.dropna(axis=0, subset=['reviewContent'])

data

"""## Buat fungsi-fungsi preprocessing"""

#Casefolder
import re

# Buat fungsi untuk langkah case folding
def casefolding(text):
  text = text.lower()                               # Mengubah teks menjadi lower case
  text = re.sub(r'https?://\S+|www\.\S+', '', text) # Menghapus URL
  text = re.sub(r'[-+]?[0-9]+', '', text)           # Menghapus angka
  text = re.sub(r'[^\w\s]','', text)                # Menghapus karakter tanda baca
  text = text.strip()
  return text

#Normalization
!wget https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/data/key_norm.csv

key_norm = pd.read_csv('key_norm.csv')

def text_normalize(text):
  text = ' '.join([key_norm[key_norm['singkat'] == word]['hasil'].values[0] if (key_norm['singkat'] == word).any() else word for word in text.split()])
  text = str.lower(text)
  return text

#Filter
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
stopwords_ind = stopwords.words('indonesian')

more_stopwords= ['lazada','Lazada']
stopwords_ind= stopwords_ind+more_stopwords

def remove_stop_words(text):
  clean_words = []
  text = text.split()
  for word in text:
      if word not in stopwords_ind:
          clean_words.append(word)
  return " ".join(clean_words)

#Stemmer
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Buat fungsi untuk langkah stemming bahasa Indonesia
def stemming(text):
  text = stemmer.stem(text)
  return text

# Fungsi gabungan
def textPrep(text):
  text = casefolding(text)
  text = text_normalize(text)
  text = remove_stop_words(text)
  text = stemming(text)
  return text

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #data['clean_review_title']=data['reviewTitle'].apply(textPrep)
# data['clean_review_content']=data['reviewContent'].apply(textPrep)

data.to_csv('cleandata (1).csv')

data=pd.read_csv('cleandatapls.csv')
#data.clean_review_content.apply(str)

data.helpful=data.helpful.replace({True:1,False:0})

data['helpful'].unique()

data['rating'].unique()

rows= data[~data['rating'].isnull()]

rows= rows[~rows['clean_review_content'].isnull()]

rows

rows=rows.sample(n=40000)

rows

"""#Feature Engineering"""

x= rows[rows['clean_review_content'].notna()]
y= rows['rating']

x=x.clean_review_content

x

x.isna().sum()

y

"""#Feature Extraction (BoW & N-Gram)"""

from sklearn.feature_extraction.text import CountVectorizer
#BoW
vec= CountVectorizer(ngram_range=(1,1))
vec.fit(x)

print(len(vec.get_feature_names_out()))

print(vec.get_feature_names_out())

X_bigram= vec.transform(x).toarray()
X_bigram

bigramData= pd.DataFrame(X_bigram,columns=vec.get_feature_names_out())
bigramData

with open('tugasbow.pickle', 'wb') as output:
  pickle.dump(X_bigram, output)

"""#Feature Extraction for TFIDF & N-Gram"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf_idf = TfidfVectorizer(ngram_range=(1,1))
tf_idf.fit(x)

X_tf_idf = tf_idf.transform(x)

print(len(tf_idf.get_feature_names_out()))

116361

print(tf_idf.get_feature_names_out())

X_tf_idf = tf_idf.transform(x).toarray()
X_tf_idf

dataTfidf=pd.DataFrame(X_tf_idf, columns=tf_idf.get_feature_names_out())
dataTfidf

with open('tf_idfTugas.pickle', 'wb') as output:
  pickle.dump(X_tf_idf, output)

#Feature Selection

X=np.array(dataTfidf)
y=np.array(y)

from sklearn.feature_selection import SelectKBest 
from sklearn.feature_selection import chi2 

# Ten features with highest chi-squared statistics are selected 
chi2_features = SelectKBest(chi2, k=1000) 
X_kbest_features = chi2_features.fit_transform(X, y) 
  
# Reduced features 
print('Original feature number:', X.shape[1]) 
print('Reduced feature number:', X_kbest_features.shape[1])

data_chi2=pd.DataFrame(chi2_features.scores_,columns=['nilai'])
data_chi2

# Menampilkan fitur beserta nilainya
feature = tf_idf.get_feature_names_out()
data_chi2['fitur'] = feature
data_chi2

# Mengurutkan fitur terbaik
data_chi2.sort_values(by='nilai', ascending=False)

# Menampilkan mask pada feature yang diseleksi
# False berarti fitur tidak terpilih dan True berarti fitur terpilih
mask = chi2_features.get_support()
mask

# Menampilkan fitur-fitur terpilih berdasarkan mask atau nilai tertinggi yang sudah dikalkulasi pada Chi-Square
new_feature = []
for bool, f in zip(mask, feature):
  if bool:
    new_feature.append(f)
  selected_feature = new_feature
len(selected_feature)

data_selected_feature = pd.DataFrame(X_kbest_features, columns=selected_feature)
data_selected_feature

#WordCloud

!pip install wordcloud

import cv2
from wordcloud import WordCloud

